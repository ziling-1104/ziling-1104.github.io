const URL = "https://teachablemachine.withgoogle.com/models/YOUR_MODEL_URL/"; // â† æ”¹æˆä½ çš„ TM æ¨¡å‹ç¶²å€
let model, webcam, maxPredictions;
let videoElement;
let useLowLoad = false;
let speechEnabled = false;
let detecting = false;
let lastEmotion = "";
const counts = { happy: 0, angry: 0, tired: 0, neutral: 0 };

async function init() {
  const modelURL = URL + "model.json";
  const metadataURL = URL + "metadata.json";
  model = await tmImage.load(modelURL, metadataURL);

  webcam = new tmImage.Webcam(400, 400, true); // width, height, flip
  await webcam.setup();
  await webcam.play();
  document.getElementById("webcam-container").appendChild(webcam.canvas);

  videoElement = webcam.webcam; // é€™æ˜¯ <video> å…ƒç´ 
  setupMediaPipe();

  window.requestAnimationFrame(loop);
}

function toggleSpeech() {
  speechEnabled = !speechEnabled;
  document.getElementById("speech-toggle").textContent = speechEnabled ? "ğŸ”‡ èªéŸ³é—œé–‰" : "ğŸ”Š èªéŸ³é–‹å•Ÿ";
}

function toggleLowLoad() {
  useLowLoad = !useLowLoad;
  document.getElementById("load-toggle").textContent = useLowLoad ? "âš™ï¸ ç¯€èƒ½æ¨¡å¼" : "âš™ï¸ å…¨åŠŸèƒ½æ¨¡å¼";
}

async function loop() {
  webcam.update();
  if (!detecting) {
    detecting = true;
    await detectAngry();
    detecting = false;
  }
  window.requestAnimationFrame(loop);
}

async function detectAngry() {
  const prediction = await model.predict(webcam.canvas);
  const angry = prediction.find(p => p.className === "angry");
  if (angry && angry.probability > 0.85) {
    showResult("angry", "ğŸ˜ ", "æ·±å‘¼å¸ä¸€ä¸‹ï¼Œå†·éœä¸€ä¸‹å¿ƒæƒ…å§ï¼");
  }
}

function showResult(emotion, emoji, suggestion) {
  if (emotion !== lastEmotion) {
    lastEmotion = emotion;
    counts[emotion]++;
    document.getElementById("emoji").textContent = emoji;
    document.getElementById("suggestion").textContent = suggestion;
    updateHistory(emotion, emoji);
    updateChart();
    if (speechEnabled) speakSuggestion(suggestion);
  }
}

function updateHistory(emotion, emoji) {
  const history = document.getElementById("history");
  const div = document.createElement("div");
  div.textContent = `${new Date().toLocaleTimeString()} - ${emoji} ${emotion}`;
  history.prepend(div);
}

function updateChart() {
  for (let emotion in counts) {
    const bar = document.querySelector(`.bar[data-emotion="${emotion}"]`);
    if (bar) {
      bar.style.width = counts[emotion] * 20 + "px";
      bar.textContent = counts[emotion];
    }
  }
}

function speakSuggestion(text) {
  const utter = new SpeechSynthesisUtterance(text);
  utter.lang = "zh-TW";
  speechSynthesis.speak(utter);
}

function setupMediaPipe() {
  const faceMesh = new FaceMesh({
    locateFile: file => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`,
  });

  faceMesh.setOptions({
    maxNumFaces: 1,
    refineLandmarks: true,
    minDetectionConfidence: 0.5,
    minTrackingConfidence: 0.5,
  });

  faceMesh.onResults(onResults);

  const camera = new Camera(videoElement, {
    onFrame: async () => {
      await faceMesh.send({ image: videoElement });
    },
    width: 400,
    height: 400
  });
  camera.start();
}

function onResults(results) {
  if (!results.multiFaceLandmarks || results.multiFaceLandmarks.length === 0) return;

  const landmarks = results.multiFaceLandmarks[0];
  const leftEye = landmarks[159];
  const rightEye = landmarks[386];
  const topLip = landmarks[13];
  const bottomLip = landmarks[14];

  const eyeOpenness = Math.abs(leftEye.y - rightEye.y);
  const mouthOpen = Math.abs(topLip.y - bottomLip.y);

  if (lastEmotion === "angry") return; // è‹¥æ˜¯ç”Ÿæ°£ï¼Œæš«ä¸è®“ MediaPipe è“‹æ‰

  if (mouthOpen > 0.05) {
    showResult("happy", "ğŸ˜„", "ä½ çœ‹èµ·ä¾†å¾ˆé–‹å¿ƒï¼");
  } else if (eyeOpenness < 0.015) {
    showResult("tired", "ğŸ¥±", "æ„Ÿè¦ºæœ‰é»ç–²å€¦ï¼Œä¼‘æ¯ä¸€ä¸‹å§ï¼");
  } else {
    showResult("neutral", "ğŸ™‚", "ä½ ç¾åœ¨å¾ˆå¹³éœå–”ã€‚");
  }
}
